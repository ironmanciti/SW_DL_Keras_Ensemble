{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRqRCM_jblTz"
   },
   "source": [
    "# Overfitting & Regularization\n",
    "\n",
    "- IMBD 를 이용하여 overfitting 과 regularization test  \n",
    "- Colab 에서는 메모리 부족 발생하므로 Local 에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NepO63jmblT4"
   },
   "source": [
    "### Multi-hot-encoding\n",
    "\n",
    "Multi-hot encoding은 텍스트 데이터를 벡터 형식으로 변환하는 방법 중 하나입니다. 주로 단어의 집합 또는 어휘를 다룰 때 사용됩니다.\n",
    "\n",
    "Multi-hot encoding의 결과를 설명하겠습니다:\n",
    "\n",
    "1. **어휘 크기**: 우선 어휘의 전체 크기, 즉 데이터에 있는 모든 고유한 단어의 수를 기반으로 고정 길이의 벡터를 설정합니다. 예를 들어, 어휘에 10,000개의 고유한 단어가 있다면 각 문서나 시퀀스를 나타내는 벡터의 길이도 10,000이 됩니다.\n",
    "\n",
    "2. **벡터화**: 각 문서나 시퀀스는 이 10,000개의 요소를 가진 벡터로 표현됩니다. 문서에 특정 단어가 포함되어 있다면, 해당 단어의 인덱스에 해당하는 벡터의 위치에 1을 설정합니다. 그렇지 않으면 0을 설정합니다.\n",
    "\n",
    "예를 들어:\n",
    "- 어휘: [\"apple\", \"banana\", \"cherry\", \"date\", \"fig\"]\n",
    "- 문서: \"apple cherry\"\n",
    "\n",
    "이 문서를 multi-hot encoding으로 벡터화하면 다음과 같습니다:\n",
    "- 벡터: [1, 0, 1, 0, 0]\n",
    "\n",
    "Multi-hot encoding의 결과는 각 문서나 시퀀스를 고정 길이의 벡터로 변환하는 것입니다. 이 벡터는 주어진 어휘에 있는 각 단어의 존재 유무를 나타냅니다. 이 방식은 원-핫 인코딩과 유사하지만, 하나 이상의 1 값을 가질 수 있기 때문에 \"multi-hot\"이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbNakosEblT4"
   },
   "source": [
    "다음의 요령으로 train_data, test_data 를 **multi-hot-encoding** 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHw8NupbblT5"
   },
   "source": [
    "## 3 가지 크기의 model 을 만들어 underfitting, overfitting 을 비교\n",
    "\n",
    "- metrics = ['accuracy', 'binary_crossentropy'] 로 주고 history.histroy['val_binary_crossentropy'] 를 서로 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW5q6wFTblT6"
   },
   "source": [
    "### 한 figure 에 model_1,2,3 를 모두 그리고, 각각의 train / validation loss 를 같은 색으로 그린다.\n",
    "\n",
    "- history.epoch : epoch 수  \n",
    "\n",
    "\n",
    "- history.history['binary_crossentropy'] : train loss  \n",
    "\n",
    "\n",
    "- history.history['val_binary_crossentropy'] : validation loss  \n",
    "\n",
    "\n",
    "- plt.plot return value 의 get_color() 를 이용해 같은 색으로 그릴 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvxqdI2qblT6"
   },
   "source": [
    "## l2 regularization 을 적용하여 같은 Test\n",
    "\n",
    "L2 regularization은 머신 러닝 및 딥 러닝에서 모델의 가중치에 제약을 부과하여 오버피팅을 방지하는 기술입니다. 이는 모델의 복잡성을 제한하며, 일반화 능력을 향상시키는 데 도움이 됩니다.\n",
    "\n",
    "L2 regularization을 사용할 때, 손실 함수에 가중치의 제곱에 비례하는 항이 추가됩니다. 이는 모델이 학습하는 동안 가중치가 큰 값을 가지게 되면 높은 페널티가 부과되도록 하는 역할을 합니다.\n",
    "\n",
    "아래는 L2 regularization에 대한 자세한 설명입니다:\n",
    "\n",
    "1. **식**:\n",
    "$$J_{\\text{regularized}} = J + \\lambda \\sum_{i} w_{i}^{2}$$\n",
    "여기서 \\( J \\)는 원래의 손실 함수, \\( w \\)는 모델의 가중치, 그리고 $\\lambda$는 regularization strength를 의미합니다.  \n",
    "\n",
    "\n",
    "2. **정규화 강도**:\n",
    "`keras.regularizers.l2(0.001)`에서 `0.001`은 $\\lambda$ 값, 즉 regularization strength를 나타냅니다. 이 값이 크면 가중치에 대한 제약이 강화되며, 값이 작으면 제약이 약화됩니다.\n",
    "\n",
    "\n",
    "3. **장점**:   \n",
    "   - 가중치의 큰 값을 갖는 것에 페널티를 부과하여 오버피팅을 방지합니다.\n",
    "   - 모델의 일반화 능력을 향상시킵니다.\n",
    "   - 피처 간의 공선성 문제를 완화시킵니다.  \n",
    "   \n",
    "\n",
    "4. **사용 방법**:\n",
    "Keras에서 L2 regularization을 사용하려면, 해당 regularizer 객체를 레이어의 `kernel_regularizer` 인자로 전달하면 됩니다.\n",
    "```python\n",
    "layer = keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "```\n",
    "\n",
    "이렇게 사용하면, 해당 레이어의 가중치에 L2 regularization이 적용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAYIC0KoblT7"
   },
   "source": [
    "## Dropout 을 적용하여 같은 Test"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
